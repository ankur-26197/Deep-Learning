{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii6XChEtM36m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(query, key, value):\n",
        "  temp = query.bmm(key.transpose(1,2))\n",
        "  scale = query.size(-1) ** 0.5\n",
        "  softmax =  F.softmax(temp/scale, dim=-1)\n",
        "  return softmax.bmm(value)"
      ],
      "metadata": {
        "id": "M1DYNOjVNGXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, dim_in, dim_q, dim_k):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(dim_in, dim_q)\n",
        "    self.k = nn.Linear(dim_in, dim_k)\n",
        "    self.v = nn.Linear(dim_in, dim_k)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    return scaled_dot_product(self.q(query), self.k(key), self.value(v))"
      ],
      "metadata": {
        "id": "FZ0iam3JUZQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, dim_in, dim_q, dim_k):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.linear = nn.Linear(num_heads*dim_k, dim_in)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    return self.linear(\n",
        "        torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
        "    )"
      ],
      "metadata": {
        "id": "5V7DEB5ZVFX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_encoding(seq_len, dim_model, device = torch.device('cpu')):\n",
        "  pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1,-1,1)\n",
        "  dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1,1,-1)\n",
        "  phase = pos/(1e4 ** (dim/dim_model))\n",
        "\n",
        "  return torch.where(dim.long()%2==0, torch.sin(phase), torch.cos(phase))"
      ],
      "metadata": {
        "id": "qQ7_K36FW8dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2],[3,4]])\n",
        "b = torch.tensor([[5,6],[7,8]])"
      ],
      "metadata": {
        "id": "qXNt56fgWI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat([a,b], dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0RsepYjWlFC",
        "outputId": "857a8836-4263-4564-eadb-88fe55c691ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 5, 6],\n",
              "        [3, 4, 7, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(dim_input=512, dim_feedforward=2048):\n",
        "  print(dim_input, dim_feedforward)\n",
        "  return nn.Sequential(\n",
        "      nn.Linear(dim_input, dim_feedforward),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dim_feedforward, dim_input)\n",
        "  )"
      ],
      "metadata": {
        "id": "RlGVHiqRWr0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "  def __init__(self, sublayer, dimension, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, *tensors):\n",
        "    return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
      ],
      "metadata": {
        "id": "sCe8_dCzamUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model = 512, num_heads = 6, dim_feedforward = 2048, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "    self.attention = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.feed_forward = Residual(\n",
        "        feed_forward(dim_model, dim_feedforward),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "        )\n",
        "  def forward(self, src):\n",
        "    src = self.attention(src, src, src)\n",
        "    return self.feed_forward(src)"
      ],
      "metadata": {
        "id": "1I6_ALbtbjgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, num_layers=6, dim_model=512, num_heads=8, dim_feedforward=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "    )\n",
        "  def forward(self, src):\n",
        "    seq_len, dimension = src.size(1), src.size(2)\n",
        "    src += position_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      src = layer(src)\n",
        "\n",
        "    return src"
      ],
      "metadata": {
        "id": "_ghVchwgc8Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model=512, num_heads=6, dim_feedforward=2048,\n",
        "               dropout=0.1):\n",
        "    super().__init__()\n",
        "    dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "    self.attention_1 = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "\n",
        "    )\n",
        "    self.attention_2 = Residual(\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout\n",
        "\n",
        "        )\n",
        "    self.feed_forward = Residual(\n",
        "        feed_forward(dim_model, dim_feedforward),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "      tgt = self.attention_1(tgt, tgt, tgt)\n",
        "      tgt = self.attention_2(tgt, memory, memory)\n",
        "      return self.feed_forward(tgt)"
      ],
      "metadata": {
        "id": "yPjGG75BeYmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, num_layers=6, dim_model=512, num_heads=8, dim_feedforward=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "            for _ in range(num_layers)\n",
        "         ]\n",
        "    )\n",
        "    self.linear = nn.Linear(dim_model, dim_model)\n",
        "  def forward(self, tgt, memory):\n",
        "    seq_len, dimension = tgt.size(1), tgt.size(2)\n",
        "    tgt += position_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      tgt = layer(tgt,memory)\n",
        "\n",
        "    return torch.softmax(self.linear(tgt), sim=-1)"
      ],
      "metadata": {
        "id": "0Vf0YpFYg0oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_encoder_layers=6, num_decoder_layers = 6, dim_model=6, num_heads=6, dim_feedforward=2048, dropout=0.1,\n",
        "               activation = nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.encoder = TransformerEncoder(\n",
        "        num_layers = num_encoder_layers,\n",
        "        dim_model = dim_model,\n",
        "        num_heads = num_heads,\n",
        "        dim_feedforward = dim_feedforward,\n",
        "        dropout = dropout\n",
        "    )\n",
        "\n",
        "    self.decoder = TransformerDecoder(\n",
        "      num_layers = num_encoder_layers,\n",
        "      dim_model = dim_model,\n",
        "      num_heads = num_heads,\n",
        "      dim_feedforward = dim_feedforward,\n",
        "      dropout = dropout\n",
        "  )\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    return self.decoder(tgt, self.encoder(src))"
      ],
      "metadata": {
        "id": "blaOqKsehjZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.rand(64,32,512)\n",
        "tgt = torch.rand(64,16,512)\n",
        "out = Transformer()(src, tgt)"
      ],
      "metadata": {
        "id": "rdIl2sFPjDLf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4l49LnnKky9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}